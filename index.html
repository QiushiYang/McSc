<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://qiushiyang.github.io/">Qiushi Yang</a>,
            </span>
            <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=3fmmfg8AAAAJ">Yingjie Chen</a>,
              </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=mpwXqNoAAAAJ&hl=zh-CN">Yuan Yao</a>,
            </span>
            <span class="author-block">
                <a href="https://menyifang.github.io">Yifang Men</a>,
              </span>
              <span class="author-block">
                <a href="https://www.researchgate.net/profile/Huaizhuo-Liu">Huaizhuo Liu</a>,
              </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=C-7UhS9dBroC&hl=zh-CN">Miaomiao Cui</a>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Tongyi Lab, Alibaba</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://github.com/QiushiYang/McSc" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/QiushiYang/McSc" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/QiushiYang/McSc" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero">
    <div class="image-container">
        <img src="static/images/Visualization_2_more_new.jpg" alt="Qualitative comparison" style="max-width: 100%; height: auto; margin: 20px 0;">
    </div>
</section> -->
<section class="hero">
    <div class="container">
        <div class="image-container">
            <figure class="image">
                <img src="static/images/Visualization_Comparison.jpg" alt="Qualitative comparison" style="max-width: 100%; height: auto; margin: 20px 0;">
                <figcaption style="text-align: center; max-width: 100%; margin: 10px 0;">
                    Visualization comparison of generated videos by the baselines and the proposed method. Our McSc generate videos with larger motion dynamic and stronger semantic alignment.
                </figcaption>
            </figure>
        </div>
    </div>
</section>

<!-- Abstract部分保持不变 -->
<section class="section">
    <div class="container">
        <div class="content">
            <!-- <h2 class="title">Abstract</h2> -->
            <h2 class="title has-text-centered">Abstract</h2>
            <div class="content has-text-justified">
                <p>
                    Text-to-video (T2V) generation has achieved remarkable progress in producing high-quality videos aligned with textual prompts. 
                    However, aligning synthesized videos with nuanced human preference remains challenging due to the subjective and multifaceted nature of human judgment. 
                    Existing video preference alignment methods rely on costly human annotations or utilize proxy metrics to predict preference, which lacks the understanding of human preference logic. 
                    Moreover, they usually directly align T2V models with the overall preference distribution, ignoring potential conflict dimensions like motion dynamics and visual quality, which may bias models towards low-motion content. 
                    To address these issues, we present Motion-corrective alignment with Self-critic hierarchical Reasoning (McSc), a three-stage reinforcement learning framework for robust preference modeling and alignment. 
                    Firstly, Self-critic Dimensional Reasoning (ScDR) trains a generative reward model (RM) to decompose preferences into per-dimension assessments, using self-critic reasoning chains for reliable learning. 
                    Secondly, to achieve holistic video comparison, we introduce Hierarchical Comparative Reasoning (HCR) for structural multi-dimensional reasoning with hierarchical reward supervision. 
                    Finally, using RM-preferred videos, we propose Motion-corrective Direct Preference Optimization (McDPO) to optimize T2V models, while dynamically re-weighting alignment objective to mitigate bias towards low-motion content. 
                    Experiments show that McSc achieves superior performance in human preference alignment and generates videos with high-motion dynamic.
                </p>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container">
        <div class="content">
            <!-- <h2 class="title">Abstract</h2> -->
            <h2 class="title has-text-centered">Motivation</h2>
            <div class="image-container">
                <img src="static/images/Motivation_new.jpg" alt="Motivation overview" style="max-width: 100%; height: auto; margin: 20px 0;">
                    <p>
                        This work aims to address the challenge of aligning text-to-video generation models with human preferences. Traditional preference alignment methods rely heavily on large-scale human-annotated preference data, which is both time-consuming and labor-intensive. To mitigate this, recent studies have proposed a two-stage framework: preference prediction followed by preference alignment. In this framework, a preference model is first used to automatically estimate human preferences for generated videos, and the resulting pseudo-preferences are then leveraged to optimize the generative model. Building upon this paradigm, our work introduces a novel preference prediction approach, ScHR, comprising two sequential training stages, ScDR and HCR, followed by a new strategy for preference alignment.
                    </p>
                    <p>
                        Specifically, to improve both preference prediction and alignment, we draw inspiration from the way humans naturally evaluate video quality. Humans typically decompose their assessment into multiple dimensions (e.g., motion dynamics, visual quality, temporal consistency), evaluate each dimension independently, and then integrate these multi-dimensional judgments into an overall preference score. We design our training process to mimic this cognitive mechanism: the model first learns the reasoning behind human judgments along individual dimensions (ScDR), and then acquires the ability to synthesize cross-dimensional evaluations into a holistic preference (HCR).                </p>
                    <p>
                        In the course of our analysis, we uncover an unexpected yet critical phenomenon: some of dimensions involved in human preference annotations exhibit significant contradictions and negative correlations. Notably, we observe a strong negative correlation between motion dynamics and static visual quality dimensions—videos with lower motion dynamics tend to score higher on other static quality metrics, leading human annotators to rate them more favorably overall. However, using such preferences for model optimization biases the generator toward producing videos with reduced motion dynamics, ultimately compromising motion richness and diversity. To address this issue, we propose McDPO, a novel alignment strategy that explicitly mitigates this trade-off and encourages the generation of videos with enhanced motion dynamics.                </p>
            </div>
        </div>
    </div>
</section>

<!-- Method部分添加图片 -->
<section class="section">
    <div class="container">
        <div class="content">
            <!-- <h2 class="title">Method</h2> -->
            <h2 class="title has-text-centered">Method</h2>
            <div class="image-container">
                <img src="static/images/Framework_new.jpg" alt="Method overview" style="max-width: 100%; height: auto; margin: 20px 0;">
                    <p>
                        We propose McSc for video generation, integrating human preference reasoning and preference alignment to synthesis videos with estimated preference. McSc contains three key steps: (1) ScDR trains a generative reward model with a self-critic strategy towards single-dimension preference reasoning, (2) HCR exploits holistic video assessment with structured reward mechanisms, and (3) McDPO optimizes the video generation model to synthesize diverse videos align with true human preference by reducing evaluation dimension bias.                    </p>
                    <p>
            </div>
        </div>
    </div>
</section>

<!-- Video部分 -->
<section class="section">
    <div class="container">
        <div class="content">
            <h2 class="title has-text-centered">Generated Videos</h2>
            
            <!-- First row of videos -->
            <div style="display: flex; justify-content: space-between; margin: 20px 0;">
                <div style="width: 48%; text-align: center;">
                    <p style="text-align: center; margin-bottom: 10px;">Baseline: "In a still frame, a stop sign"</p>
                    <video controls style="width: 100%;">
                        <source src="static/videos/case1-baseline.mp4" type="video/mp4">
                        <!-- Your browser does not support the video tag. -->
                    </video>
                </div>
                <div style="width: 48%; text-align: center;">
                    <p style="text-align: center; margin-bottom: 10px;">McSc: "In a still frame, a stop sign"</p>
                    <video controls style="width: 100%;">
                        <source src="static/videos/case1-ours.mp4" type="video/mp4">
                        <!-- Your browser does not support the video tag. -->
                    </video>
                </div>
            </div>

            <!-- Second row of videos -->
            <div style="display: flex; justify-content: space-between; margin: 20px 0;">
                <div style="width: 48%; text-align: center;">
                    <p style="text-align: center; margin-bottom: 10px;">Baseline: "Macro slo-mo. Slow motion cropped closeup of roasted coffee beans falling into an empty bowl"</p>
                    <video controls style="width: 100%;">
                        <source src="static/videos/case2-baseline.mp4" type="video/mp4">
                        <!-- Your browser does not support the video tag. -->
                    </video>
                </div>
                <div style="width: 48%; text-align: center;">
                    <p style="text-align: center; margin-bottom: 10px;">McSc: "Macro slo-mo. Slow motion cropped closeup of roasted coffee beans falling into an empty bowl"</p>
                    <video controls style="width: 100%;">
                        <source src="static/videos/case2-ours.mp4" type="video/mp4">
                        <!-- Your browser does not support the video tag. -->
                    </video>
                </div>
            </div>

            <!-- Thrid row of videos -->
            <div style="display: flex; justify-content: space-between; margin: 20px 0;">
                <div style="width: 48%; text-align: center;">
                    <p style="text-align: center; margin-bottom: 10px;">Baseline: "A drone view of celebration with Christmas tree and fireworks, starry sky - background."</p>
                    <video controls style="width: 100%;">
                        <source src="static/videos/case3-baseline.mp4" type="video/mp4">
                        <!-- Your browser does not support the video tag. -->
                    </video>
                </div>
                <div style="width: 48%; text-align: center;">
                    <p style="text-align: center; margin-bottom: 10px;">McSc: "A drone view of celebration with Christmas tree and fireworks, starry sky - background."</p>
                    <video controls style="width: 100%;">
                        <source src="static/videos/case3-ours.mp4" type="video/mp4">
                        <!-- Your browser does not support the video tag. -->
                    </video>
                </div>
            </div>

            <!-- Fourth row of videos -->
            <div style="display: flex; justify-content: space-between; margin: 20px 0;">
                <div style="width: 48%; text-align: center;">
                    <p style="text-align: center; margin-bottom: 10px;">Baseline: "a shark is swimming in the ocean, racking focus"</p>
                    <video controls style="width: 100%;">
                        <source src="static/videos/case4-baseline.mp4" type="video/mp4">
                        <!-- Your browser does not support the video tag. -->
                    </video>
                </div>
                <div style="width: 48%; text-align: center;">
                    <p style="text-align: center; margin-bottom: 10px;">McSc: "a shark is swimming in the ocean, racking focus"</p>
                    <video controls style="width: 100%;">
                        <source src="static/videos/case4-ours.mp4" type="video/mp4">
                        <!-- Your browser does not support the video tag. -->
                    </video>
                </div>
            </div>

        </div>
    </div>
</section>

<!-- Results部分 -->
<!-- <section class="section">
    <div class="container">
        <div class="content">
            <h2 class="title has-text-centered">Results</h2>
            <div class="image-container">
                <div class="columns is-centered">
                    <div class="column is-two-thirds">
                        <img src="static/images/Results_over_baseline.jpg" alt="Results comparison" style="max-width: 100%; height: auto; margin: 20px 0;">
                        <p class="has-text-centered" style="max-width: 100%; margin: 10px 0;">
                            Performance comparison between the baseline SAM2 and MoSAM across various model sizes, including Tiny (-T), Small (-S), Base (-B+) and Large (-L). Result gains over the baseline by our method are in red.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section> -->


<section class="section" id="BibTeX">
    <div class="container content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{arxiv_coming_soon
    author    = {Qiushi Yang, Yingjie Chen, Yuan Yao, Yifang Men, Huaizhuo Liu, Miaomiao Cui},
    title     = {McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning},
    journal   = {arXiv},
    year      = {2025},
  }</code></pre>
    </div>
  </section>
